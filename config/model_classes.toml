[dit]
mmdit = [ # sd3, auraflow, pixart_s, hunyuan
    "adaLN_modulation",
    "mlp.fc",
    "mlpX",
    "w1q",
    "self_attn.out_proj",
    "w1q",
    "w1o",
    "mlpX.c_proj",
    "mlpC.c_proj",
    "w2q.",
    "w2k.",
    "w2v.",
    "w2o.",
    "w1k.",
    "w1v.",
    "mlpX.c_fc",
    "mlpX.c_proj.",
    "mlpC.c_fc",
    "modC.",
    "modX.",
]

auraflow = [ 
    "model.register_tokens",
    "model.positional_encoding",
    "model.init_x_linear", #weight/bias,
    "model.t_embedder.mlp.",
    "model.t_embedder.mlp.",
]

flux = [
    "modCX",
    "img_attn.proj",
    "time_in.in_layer", # suffix (.bias/.weight)
    "time_in.out_layer",
    "vector_in.in_layer",
    "vector_in.out_layer",
    "guidance_in.in_layer",
    "guidance_in.in_layer",
    "txt_in",
    "img_in",
    "img_mod.lin",
    "txt_mod.lin",
    "img_attn.qkv",
    "txt_attn.qkv",
]

pixart_s = [
    "t_embedder.mlp",
    "y_embedder.embedding_table",
]

hunyuan = [ # prefix (attn1/attn2/weight/bias)
    "wkqv.",
    "wqkv.",
    "q_norm",
    "k_norm",
    "out_proj",
    "kq_proj",
    "default_modulation.",
    "pooler",
    "t_embedder",  #TimestepEmbedding
    "x_embedder",
    "mlp_t5",
    "time_extra_emb.extra_embedder",
]

diffusers = [ # prefix (attn1/attn2) suffix(/weight/bias)
    "to_q",
    "to_k",
    "to_v",
    "norm_q",
    "norm_k",
    "to_out",
    "norm1.norm",
    "norm1.linear",
    "ff.net.0",
    "ff.net.2",
    "time_extra_emb", # TimestepEmbedding
    "time_embedding",
    "pos_embd",
    "text_embedder",
    "extra_embedder",
    "attn.norm_added_q",
]

sdxl = [
    "skip.connection",
    "upsamplers",
    "downsamplers",
    "op",
    "in.layers.2",
    "out.layers.3",
]

sd = [
    "in_layers",
    "out_layers",
    "emb_layers",
    "skip_connection",
]

diffusers_lora = [
    "text_model",
    "self_attn",
    "to_q_lora",
    "to_k_lora",
    "to_v_lora",
    "to_out_lora",
    "text_projection",
]

unet_lora = [
    "to.q.lora",
    "to.k.lora",
    "to.v.lora",
    "to.out.0.lora",
    "proj.in",
    "proj.out",
    "emb.layers",
]

[text]
transformers = [
    "norm1.",
    "norm2.",
    "norm3.",
    "attn1.to_q.",
    "attn1.to_k.",
    "attn1.to_v.",
    "attn1.to_out.0.",
    "attn2.to_q.",
    "attn2.to_k.",
    "attn2.to_v.",
    "attn2.to_out.0.",
    "ff.net.0.proj.",
    "ff.net.2.",
]

unet = [
    "proj_in.",
    "proj_out.",
    "norm.",
]